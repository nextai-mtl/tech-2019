{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9ALu78bhPXM"
   },
   "source": [
    "# NEXTAI-MTL\n",
    "# Technical Stream\n",
    "# 2019 Cohort\n",
    "# Tutorial : Recurrent neural networks (RNNs)\n",
    "## Tutorial was adapted from this one (in french): \n",
    "https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/RNN/RNN_solutions.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MIH_PfZV1rNa"
   },
   "source": [
    "## Autors\n",
    "\n",
    "Francis Grégoire <francis.gregoire@rd.mila.quebec>\n",
    "\n",
    "Jeremy Pinto <jeremy.pinto@rd.mila.quebec>\n",
    "\n",
    "Jean-Philippe Reid <Jean-Philippe.Reid@ElementAI.com>\n",
    "\n",
    "Mirko Bronzi <mirko.bronzi@mila.quebec>\n",
    "\n",
    "### Translation to English: \n",
    "\n",
    "Laurent Charlin <lcharlin@gmail.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYrPFO1p1rX1"
   },
   "source": [
    "## Preface\n",
    "\n",
    "This tutorial introduces the fundamental concepts that underlie recurrent neural networks (RNN et LSTM) using two example tasks.\n",
    "\n",
    "The first task builds and compares an LSTM model to an RNN model.\n",
    "\n",
    "In the second task, we exploit the properties of an LSTM by developping a neural language model to generate text. Through this example, you will learn how to preprocess text data to efficiently train a neural language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ArGnixElhPXN"
   },
   "source": [
    "---\n",
    "# Initialization \n",
    "\n",
    "To ensure that this tutorial runs properly on the Colab environment, we must install a few libraries using the `pip` utility. \n",
    "\n",
    "To begin, ensure that you are \"connected\" to the notebook ( check for \"✓ CONNECTED\" at the top right of your window). Then execute the cell below by selecting it and clicking `shift`+`Enter`. This can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "id": "028-EhOGhPXO",
    "outputId": "d4754a3b-5953-4c8d-f1bd-0f75ba16561c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached https://files.pythonhosted.org/packages/5f/4a/f4eb1d57fa7db52298959ce38f27869dfe6bc75010048a64c5be94cbd33f/torch-1.0.1.post2-cp37-none-macosx_10_7_x86_64.whl\n",
      "Collecting torchvision\n",
      "  Using cached https://files.pythonhosted.org/packages/fb/01/03fd7e503c16b3dc262483e5555ad40974ab5da8b9879e164b56c1f4ef6f/torchvision-0.2.2.post3-py2.py3-none-any.whl\n",
      "Collecting matplotlib\n",
      "  Using cached https://files.pythonhosted.org/packages/2e/81/bb51214944e79f9c9261badd7ef99b573fb0bc9110c0075c6a9e76224d0d/matplotlib-3.0.3-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting pillow>=4.1.1 (from torchvision)\n",
      "  Using cached https://files.pythonhosted.org/packages/c9/ed/27cc92e99b9ccaa0985a66133baeea7e8a3371d3c04cfa353aaa3b81aac1/Pillow-5.4.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting numpy (from torchvision)\n",
      "  Using cached https://files.pythonhosted.org/packages/a6/6f/cb20ccd8f0f8581e0e090775c0e3c3e335b037818416e6fa945d924397d2/numpy-1.16.2-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting six (from torchvision)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting python-dateutil>=2.1 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/de/0a/001be530836743d8be6c2d85069f46fecf84ac6c18c7f5fb8125ee11d854/pyparsing-2.3.1-py2.py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/68/f2/21ec13269a420c063a3d7d8c87dac030da7b00fc6b27fa88cfb1c72a645b/kiwisolver-1.0.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (40.6.3)\n",
      "Installing collected packages: torch, pillow, numpy, six, torchvision, cycler, python-dateutil, pyparsing, kiwisolver, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.0.1 matplotlib-3.0.3 numpy-1.16.2 pillow-5.4.1 pyparsing-2.3.1 python-dateutil-2.8.0 six-1.12.0 torch-1.0.1.post2 torchvision-0.2.2.post3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "UwebZdYMhPXT",
    "outputId": "dce7f788-6635-4ad6-f85a-65118c45e7c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:  1.0.1.post2\n",
      "GPU available: False\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
    "\n",
    "# Setting the seed to a fixed value can be helpful in reproducing results\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "print(\"PyTorch version: \", torch.__version__)\n",
    "print(\"GPU available: {}\".format(use_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "odAnur2xtX8n"
   },
   "source": [
    "---\n",
    "# Theoretical context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bU1k5WvhhPXX"
   },
   "source": [
    "\n",
    "## Recurrent neural networks\n",
    "\n",
    "Sequential data is ubiquituous and many tasks require it. Models of text data for tasks such as automatic translation, question answering, and dialogs are good examples. These model take into account the sequentiality (or temporality) of the data. That is, they model each word using its surrounding context (most often the words that appeared previously). \n",
    "\n",
    "Sequential data is challenging for multi-layer perceptrons because the data is typically not of fixed length (i.e., different text have different lengths). Further MLPs typically do not model the order of their inputs while the order of sequential data is very important.\n",
    "\n",
    "Recurrent neural networks (RNNs) are models designed for such varying-length data. By varying length we mean that each example may have a different length (for example if each example is a sentence, then sentences could have different lengths). More formally, two sentences $\\mathbf{s}_{i} = (x_{i,1}, \\dots, x_{i,N})$ and $\\mathbf{s}_{j} = (x_{j,1}, \\dots, x_{j,M})$ are of different lengths if $N \\neq M$. The hidden layer of an RNN  $\\mathbf{h}_{t} \\in \\mathbb R^{d_{h}}$, also known as a recurrent state, is a $d_{h}$-dimensional vector which represents the memory of the network through time. At each time step $t$, the recurrent state is a recursive function which updates itself using the input variables $x_{t}$ and its recurrent state at the previous timestep $\\mathbf{h}_{t-1}$. This function can be written down as\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathbf{h_{t}} = f(x_{t}, \\mathbf{h}_{t-1}).\n",
    "\\end{equation}\n",
    "\n",
    "It ensures that the output of the model at each timestep $t$ depends on the history (e.g., all inputs up to now). The function $f(\\cdot)$ can be parametrized in various ways. For a vanilla RNN we use: \n",
    "\n",
    "\\begin{equation}\n",
    "  f(x_{t}, \\mathbf{h}_{t-1}) = g(\\mathbf{W}\\psi(x_{t}) + \\mathbf{U}\\mathbf{h}_{t-1} + \\mathbf{b}_{h}),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\psi(x_{t})$ can be the identity function (this is what we will use in task 1) or a function which transforms a discrete variable into a continuous $d_{e}$-dimensional vectorial representation (this is what we will use in task 2). The $\\mathbf{W} \\in \\mathbb R^{d_{h} \\times d_{e}}$ and $\\mathbf{U} \\in \\mathbb R^{d_{h} \\times d_{h}}$ matrices are the shared parameters of the model. They are shared through all timesteps. (In other words we assume that the transition dynamics are static). These matrices encode how much importance to give to the current datum and to the previous recurrent state. Last, $g(\\cdot)$ is a non-linear activation.\n",
    "\n",
    "A schema of a basic RNN looks like this:\n",
    "\n",
    "![alt-text](https://github.com/nextai-mtl/tech-2019/blob/master/images/RNN-unrolled.png?raw=true)\n",
    "\n",
    "The network's structure and the output $y$'s format are task-dependant. For modelling classification tasks where the output occurs at a single timestep (usually right after the last input) we typically use the vector representation of the last recurrent state. \n",
    "\n",
    "For binary classification problems  ($y \\in \\{0,1\\}$), we use a $\\sigma$ activation function which corresponds to a Bernoulli output. This output can be understood as predicting a normalized score sometimes referred to as the probability of the output taking the value 1: \n",
    "\n",
    "\\begin{equation}\n",
    "  p(y_{t} = 1 | \\mathbf{h}_{t}) = \\sigma(\\mathbf{v}^\\top \\mathbf{h}_{t} + \\mathbf{b}_{c}).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "For multi-class classification problems ($y \\in \\{0,1,2,\\ldots,n\\}$), the output layer contains $n$ neurons and we typically use a *softmax* activation function which returns normalized score over each possible class:\n",
    "\n",
    "\\begin{equation}\n",
    "  [p(y_{t} = 1 | \\mathbf{h}_{t}), \\dots, p(y_{t} = n | \\mathbf{h}_{t})]^\\top = softmax(\\mathbf{V}^\\top \\mathbf{h}_{t} + \\mathbf{b}_{c}),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{V} \\in \\mathbb R^{d_{h} \\times d_{y}}$, $\\mathbf{v}\\ \\in \\mathbb R^{d_{h}}$ et $\\mathbf{b}_{c}\\ \\in \\mathbb R^{d_{y}}$ are parameters of the model.\n",
    "\n",
    "### Long-term dependencies\n",
    "In theory, RNNs can model arbitrarily long dependencies in sequences. In practice, it was shown that it is difficult to learn long-term dependices using RNNs (see this [article from Bengio et al. (1994)](http://ai.dinfo.unifi.it/paolo/ps/tnn-94-gradient.pdf) for the details). The intuition is as follows. RNNs are recursive models and their parameters are shared through all timesteps (e.g., for a sequence of length 10, the same parameters will be used on the order of 10 times). Let us think of how the error calculated at the last step will be lead to an update at the first step. The gradient will have to flow back through each recurent state in reverse order. This sequence of operations involves a sequence of multiplications (through the chain rule of derivation). For a sequence of $N$ timesteps, this could have two effects: 1) the gradient could be multiplied by a sequence of small numbers and tend to 0; 2) the gradient can be multiplied by a sequence of large numbers and tend to very large values. The former is called the *vanishing gradient problem*, while the latter the *exploding gradient problem*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2SmKN04AudAI"
   },
   "source": [
    "## Long Short Term Memory\n",
    "\n",
    "The Long Short Term Memory (LSTM) was introduced to sidestep the *exploding gradient problem* and in doing so to enable RNNs to learn (very) long-term dependencies. It is now the most commonly used model for natural language processing (NLP) tasks using neural architectures. LSTMs change the internal functions used to update recurrent states to a more sophisticated one. The two images below show the difference between the internal mechanism of an RNN (above) and that of a LSTM (below).\n",
    "\n",
    "![alt-text](https://github.com/nextai-mtl/tech-2019/blob/master/images/LSTM3-SimpleRNN.png?raw=true)\n",
    "![alt-text](https://github.com/nextai-mtl/tech-2019/blob/master/images/LSTM3-chain.png?raw=true)\n",
    "\n",
    "Contrary to a vanilla RNN, an LSTM divides the memory into two componants: 1) the state of the memory $\\mathbf{c_{t}}$, and 2) the recurrent state $\\mathbf{h_{t}}$. The recurrent state $\\mathbf{h_{t}}$ is a subset of the hidden-memory state $\\mathbf{c_{t}}$ and only it is visible to the other parts of the network. The LSTM uses an *output gate* $\\mathbf{o}$ to determine how much memory to let through to the recurrent state. The *output gate* function is calculated as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathbf{o} = \\sigma(\\mathbf{W}_{o} \\psi(x_{t}) + \\mathbf{U}_{o} \\mathbf{h}_{t-1} + \\mathbf{b}_{o}).\n",
    "\\end{equation}\n",
    "\n",
    "This vector is multiplied element-by-element with its memory state which results in the following recurrent state:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathbf{h}_{t} = \\mathbf{o} \\odot tanh(\\mathbf{c}_{t}).\n",
    "\\end{equation}\n",
    "\n",
    "To update the memory state, the LSTM uses a *forget gate* $\\mathbf{f}$ and an *input gate* $\\mathbf{i}$ such that:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathbf{c}_{t} = \\mathbf{f} \\odot \\mathbf{c}_{t-1} + \\mathbf{i} \\odot \\tilde{\\mathbf{c}}_{t},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\tilde{\\mathbf{c}}_{t}$ is a memory-state candidate. The *forget gate* determines how much information to forget from the memory state, while the *input gate* controles how much information to keep from the new input and from the the previous recurrent state:\n",
    "\n",
    "\\begin{gather}\n",
    "  \\mathbf{f} = \\sigma(\\mathbf{W}_{f} \\psi(x_{t}) + \\mathbf{U}_{f} \\mathbf{h}_{t-1} + \\mathbf{b}_{f}), \\\\\n",
    "  \\mathbf{i} = \\sigma(\\mathbf{W}_{i} \\psi(x_{t}) + \\mathbf{U}_{i} \\mathbf{h}_{t-1} + \\mathbf{b}_{i}), \\\\\n",
    "  \\tilde{\\mathbf{c}} = tanh(\\mathbf{W}_{c} \\psi(x_{t}) + \\mathbf{U}_{c} \\mathbf{h}_{t-1} + \\mathbf{b}_{c}).\n",
    "\\end{gather}\n",
    "\n",
    "The parameters $\\mathbf{W}_{o}$, $\\mathbf{U}_{o}$, $\\mathbf{b}_{o}$, $\\mathbf{W}_{f}$, $\\mathbf{U}_{f}$, $\\mathbf{b}_{f}$, $\\mathbf{W}_{i}$, $\\mathbf{U}_{i}$, $\\mathbf{b}_{i}$, $\\mathbf{W}_{c}$, $\\mathbf{U}_{c}$ and $\\mathbf{b}_{c}$ are specific LSTM parameters which must be learned just like all other model parameters.\n",
    "\n",
    "To know more about LSTMs we encourage you to read this [article in Colah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (this is *not* colab).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SLcPxj5z2vX"
   },
   "source": [
    "---\n",
    "# Task 1: Adding numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Y-PnqdMhPXX"
   },
   "source": [
    "\n",
    "## Objective\n",
    "\n",
    "Our goal is to build a recurrent model capable of adding an arbitrary series of numbers. It is all in all a fairly simple task (any pocket calculator can trivially do it) yet it will demonstrate that this procedure can be learned from data. Further, we will use it to outline certain limitations of RNNs compared to LSTMs. It is also a good first task since it will be easy to generate data for it and train our models (both RNN and LSTM) on this data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nihkB--rz6WL"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Our data is composed of $x$ a set of sequences of numbers of length $seq\\_len$ each associated with a target $y$ the sum of each element in each $x$. In other words, the input to our model will be a vector $\\mathbf x^{(i)} = \\left[x_{1}^{i}, x_{2}^{i}, \\dots, x_{T}^{i}\\right]$ of length $seq\\_len=T$ and a target $y^{(i)}$ given by:\n",
    "\n",
    "\\begin{align}  \n",
    "y^{(i)}=\\sum_{j=1}^{seq\\_len}x^{(i)}_j,\n",
    "\\end{align}\n",
    "\n",
    "where $j$ indexes time.\n",
    "\n",
    "For example, for the input $\\mathbf x^{(i)}$ with $seq\\_len=4$, we have:\n",
    "\n",
    "\\begin{align}  \n",
    "\\mathbf x^{(i)} &= \\left[ 4,-1,15,24\\right], \\, \\mathbf x^{(i)} \\in \\mathbb R^{4}; \\\\ \n",
    "y^{(i)} &= 42, \\, \\mathrm y^{(i)} \\in \\mathbb R.\n",
    "\\end{align}\n",
    "\n",
    "We will use our data to train both an RNN and an LSTM. Since the target is an unbounded integer, we will use a linear hidden layer (i.e., identity activation) to project the last recurrent state of the RNN/LSTM, $h^{(i)}_{T}$, as shown on the following figure:\n",
    "\n",
    "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.002.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zZNrVvS8hPXY"
   },
   "source": [
    "### Generating our dataset\n",
    "\n",
    "It will be useful to have a function that can generate random datasets of `n_samples` sequences each of length `seq_length`. To do so we will rely on the function [torch.randint()](https://pytorch.org/docs/stable/torch.html#torch.randint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "nKFfyWFahPXZ",
    "outputId": "8d9b7f1b-28c9-45d7-900b-2256539a1fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor dimensions X = torch.Size([1000, 4, 1])\n",
      "where n_samples = 1000, seq_len = 4, input_dim = 1\n"
     ]
    }
   ],
   "source": [
    "def generate_data(n_samples, seq_len, input_dim, xmin=-100, xmax=100):\n",
    "    \"\"\"Generate tensors X and Y within the [xmin, xmax] interval.\n",
    "    \n",
    "    Args : \n",
    "      n_samples: int, number of sequences to generate\n",
    "      seq_len: int, length of each sequence\n",
    "      xmin: minimum possible value in the sequence\n",
    "      xmax: maximum possible value in the sequence\n",
    "    \n",
    "    Returns: n_samples sequence of numbers X and associated targets Y in this format \n",
    "             torch.Tensor where X.shape = (n_samples, seq_len, 1) and\n",
    "             Y.shape = (n_samples, 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    # To complete\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "n_samples = 1000\n",
    "seq_len = 4\n",
    "input_dim = 1\n",
    "X, Y = generate_data(n_samples, seq_len, input_dim, -100, 100)\n",
    "print(\"Tensor dimensions X = {}\".format(X.shape))\n",
    "print(\"where n_samples = {}, seq_len = {}, input_dim = {}\".format(*X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-uhue_rShPXc"
   },
   "source": [
    "The `generate_data()` function returns two tensors `X` and `Y`, where `X` contains the input sequences of numbers and `Y` their associated targes (i.e., the sum of the `X` sequences). `X`'s shape is `(n_samples, seq_len, input_size)` where `input_size=1` since we will feed a single real number at each timestep $\\in \\mathbb R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1wHZ_PFhPXf"
   },
   "source": [
    "### Standardizing data\n",
    "\n",
    "To help the training of our model we will first standardize the input data such that it has, overal, mean 0 and standard deviation (std) 1. To do so we simply calculate the mean and std of our data and then substract the former and divide by the latter. In addition, we will store these two values for later usage. This standardization often speeds up learning.\n",
    "\n",
    "Note: since we generated our data from a uniform distribution, the standard deviation should be close to $\\frac{(xmax-xmin)}{\\sqrt{12}}$ while the mean should be close to $\\frac{(xmax-xmin)}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UdL_y85mhPXg",
    "outputId": "50e3339c-df96-43e1-efac-9348fcbab9b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = 0.1497, std = 58.4915\n"
     ]
    }
   ],
   "source": [
    "def standardize(X):\n",
    "    \"\"\"The function standardizes the X tensor. \n",
    "    Args:\n",
    "      X: torch.Tensor.\n",
    "    \n",
    "    Returns:\n",
    "      X: torch.Tensor standardize.\n",
    "      Y: torch.Tensor, the (new) sum of X.\n",
    "    \"\"\"\n",
    "\n",
    "    # To complete\n",
    "\n",
    "    return X, Y, mean, std\n",
    "\n",
    "\n",
    "X, Y = generate_data(n_samples, seq_len, input_dim, -100, 100)\n",
    "X, Y, mean, std = standardize(X)\n",
    "print(\"mean = {:.4f}, std = {:.4f}\".format(mean, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCm8xalthPXj"
   },
   "source": [
    "## RNN implementation \n",
    "\n",
    "We will define our RNN using the following PyTorch class [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn). For more details regarding the implementation of this class, we suggest this [tutoriel](https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#example-2-recurrent-net). Once initialized, it takes input data `X` of size `(seq_len, batch_size, input_size)` (recall that we will use `input_size=1` for our task). As explained earlier, we then add a linear layer to transform the last hidden recurrent state to have the same dimensionality as `Y` which has size `(batch_size, input_size)` (again `input_size=1`). \n",
    "\n",
    "To define the architecture of our RNN, we will use this module [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn) followed by a linear layer [torch.nn.Linear()](https://pytorch.org/docs/stable/nn.html#linear). The following methods are to be completed:\n",
    "<ul>\n",
    "<li>The `__init__()` method to define the different layers of our model. </li>\n",
    "<li>The `forward()` method which uses the layers and the input variables and returns an output (this is effectively a *forward pass*).</li>\n",
    "</ul>\n",
    "\n",
    "**Nota bene**: \n",
    "\n",
    "* It can be puzzling to obtain the last recurrent state of the last hidden layer of an RNN, $h_{T}^{N}$, where $T$ is the last timestep and $N$ is the last hidden layer. It can be obtained by indexing one of these tensor as follows: `output[-1, :, :]` or `hidden_T[-1]`.\n",
    "\n",
    "* You must ensure that the dimensions of your input data `X` matches what is required by the RNN class [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn). *Hint:* this method [tensor.transpose()](https://pytorch.org/docs/stable/tensors.html?highlight=transpose#torch.Tensor.transpose) can be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "3QCVSZGzhPXk",
    "outputId": "ea47b9d8-45d2-43c7-f68c-0b95b75a0fa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of input data: torch.Size([1000, 4, 1])\n",
      "Size of predictions: torch.Size([1000, 1])\n"
     ]
    }
   ],
   "source": [
    "class RNNLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size, n_layers):\n",
    "        super(RNNLinear, self).__init__()\n",
    "        \n",
    "        # To complete\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # To complete\n",
    "        # Recall: The RNN's input must be of size (seq_len, batch_size, input_size)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    \n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "hidden_size = 20\n",
    "\n",
    "# Declare the RNN model\n",
    "model_rnn = RNNLinear(input_dim, output_dim, hidden_size, n_layers).to(device)\n",
    "\n",
    "# \n",
    "init_rnn_weights = copy.deepcopy(model_rnn.state_dict())\n",
    "\n",
    "# Use the RNN to predict the output of each input sequence prior to training\n",
    "# Ensure that the inputs and output are correct\n",
    "X = X.to(device)\n",
    "y_pred = model_rnn(X)\n",
    "print(\"Size of input data: {}\".format(X.shape)) # (seq_len, batch_size, input_size)\n",
    "print(\"Size of predictions: {}\".format(y_pred.shape)) # (batch_size, input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jg8vsRduhPYH"
   },
   "source": [
    "## LSTM implementation\n",
    "\n",
    "We will now implement an LSTM using this PyTorch class [torch.nn.LSTM()](https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM). Just like with the RNN, we will need to add a linear layer to transform the last recurrent state of our LSTM to have the same dimensions of our target `Y` which has size `(batch_size, input_size)` (with `input_size=1` as above). For additional information regarding the implementation of this class have look at [tutoriel](https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#example-2-recurrent-net).\n",
    "\n",
    "To define the architecture of our LSTM, the following methods are to be completed:\n",
    "<ul>\n",
    "    <li>The `__init__()` method to define the different layers of our model.</li>\n",
    "<li>The `forward()` method method which uses the layers and the input variables and returns an output (this is effectively a *forward pass*). </li>\n",
    "</ul>\n",
    "\n",
    "**Nota bene**: \n",
    "\n",
    "* It can be puzzling to obtain the last recurrent state of the last hidden layer of an RNN, $h_{T}^{N}$, where $T$ is the last timestep and $N$ is the last hidden layer. It can be obtained by indexing one of these tensor as follows: `output[-1, :, :]` or `hidden_T[-1]`.\n",
    "\n",
    "* You must ensure that the dimensions of your input data `X` matches what is required by the RNN class [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn). *Hint* this method [tensor.transpose()](https://pytorch.org/docs/stable/tensors.html?highlight=transpose#torch.Tensor.transpose) can be useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "lqMaUDDVhPYH",
    "outputId": "2634cd46-dbc9-43f9-b053-92081a26941f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions initiales des données en entrée: torch.Size([1000, 4, 1])\n",
      "Dimensions des prédictions: torch.Size([1000, 1])\n"
     ]
    }
   ],
   "source": [
    "class LSTMLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_size, n_layers):\n",
    "        super(LSTMLinear, self).__init__()\n",
    "        \n",
    "        # To complete\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # To complete\n",
    "        # Recall: The RNN's input must be of size (seq_len, batch_size, input_size)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "hidden_size = 20\n",
    "\n",
    "# Declare the LSTM model\n",
    "model_lstm = LSTMLinear(input_dim, output_dim, hidden_size, n_layers).to(device)\n",
    "\n",
    "# Store the initial weights of the model\n",
    "init_lstm_weights = copy.deepcopy(model_lstm.state_dict())\n",
    "\n",
    "# Use the LSTM to predict the output of each input sequence prior to training\n",
    "# Ensure that the inputs and output are correct\n",
    "X = X.to(device)\n",
    "y_pred = model_lstm(X)\n",
    "print(\"Dimensions initiales des données en entrée: {}\".format(X.shape)) # (seq_len, batch_size, input_size)\n",
    "print(\"Dimensions des prédictions: {}\".format(y_pred.shape)) # (batch_size, input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdha8WWWhPXp"
   },
   "source": [
    "## Splitting the data into train/validation/test\n",
    "\n",
    "We will use PyTorch's DataLoader objects to efficiently manipulate our data. \n",
    "\n",
    "We will generate 20,000 sequences and use 80% of those for training, 10% for validation, and 10% for testing. We can use the following functions [torch.utils.data.TensorDataset()](https://pytorch.org/docs/stable/data.html) and [torch.utils.data.DataLoader()](https://pytorch.org/docs/stable/data.html) to prepare our Dataloader.\n",
    "\n",
    "Use the following values: \n",
    "\n",
    "`seq_len = 18` \n",
    "\n",
    "`batch_size = 64`\n",
    "\n",
    "`n_samples = 25000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "PzSCKyyMhPXq"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "n_samples = 25000\n",
    "seq_len = 18\n",
    "batch_size = 64\n",
    "\n",
    "# To complete\n",
    "\n",
    "X, Y = ... # Generate data and standardize it\n",
    "\n",
    "\n",
    "xtrain, ytrain = X[:round(0.8*n_samples)], Y[:round(0.8*n_samples)]\n",
    "xvalid, yvalid = X[round(0.8*n_samples):round(0.9*n_samples)], Y[round(0.8*n_samples):round(0.9*n_samples)]\n",
    "xtest, ytest = X[round(0.9*n_samples):], Y[round(0.9*n_samples):]\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(xtrain, ytrain),\n",
    "                          batch_size, shuffle=True)\n",
    "\n",
    "# To complete\n",
    "valid_loader = ...\n",
    "\n",
    "# To complete\n",
    "test_loader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W8IdyMkZjRs7"
   },
   "source": [
    "## Training an RNN\n",
    "\n",
    "Several cost functions and optimizers can be used from PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ow4dIMYT2gek"
   },
   "source": [
    "### Defining the cost and the optimizer\n",
    "\n",
    "Recall that a cost function  $J(\\theta) = L(x, y, \\theta)$ takes as input a prediction and the target and evaluates some distance (or discrepancy) between both.  For this example, we will use the mean squared error cost which is standard for regression problems (see [torch.nn.MSELoss()](https://pytorch.org/docs/stable/nn.html)):\n",
    "\n",
    "$J(\\cdot) = \\frac{1}{N}\\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^{2}$.\n",
    "\n",
    "\n",
    "To optimize the parameters of our networks we will use the *stochastic gradient descent* (SGD) optimizer. It minimizes the cost function $J(\\theta)$ parametrized by the networks' weights $\\theta$ by updating them using the following update rule: $\\theta \\leftarrow \\theta - \\alpha \\nabla J(\\theta)$, where  $\\alpha$ is the *learning rate*. The specificity of SGD is that it will calculate the gradient $\\nabla$ using a single (or a small number of) example(s) instead of the full training data.\n",
    "\n",
    "In PyTorch we will use <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD()`</a> which is a SGD implementation. In this example, we will use a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6SidKvG6hPXu"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "# To complete\n",
    "criterion = ...\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBrLb8PdhPXw"
   },
   "source": [
    "### Training the model\n",
    "\n",
    "To train out model, we will use our `train_loader` object to iterate over our entire training sets *n_epoch* times. \n",
    "To measure progress we will store the validation cost at the end of each training *epoch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "e9a-gjjVhPXx",
    "outputId": "bb537045-efd0-4691-d7b6-92215fdb190c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epoch  1 | Training loss = 8.94425 | Validation loss = 3.35202 \n",
      "Epoch  2 | Training loss = 3.21347 | Validation loss = 2.04823 \n",
      "Epoch  3 | Training loss = 2.01104 | Validation loss = 1.11231 \n",
      "Epoch  4 | Training loss = 6.70822 | Validation loss = 9.61898 \n",
      "Epoch  5 | Training loss = 4.57141 | Validation loss = 4.72359 \n",
      "Epoch  6 | Training loss = 3.67430 | Validation loss = 1.52073 \n",
      "Epoch  7 | Training loss = 3.90616 | Validation loss = 1.95270 \n",
      "Epoch  8 | Training loss = 2.14970 | Validation loss = 1.09816 \n",
      "Epoch  9 | Training loss = 1.54088 | Validation loss = 0.70279 \n",
      "Epoch 10 | Training loss = 1.32599 | Validation loss = 2.02014 \n",
      "Epoch 11 | Training loss = 1.28169 | Validation loss = 0.57678 \n",
      "Epoch 12 | Training loss = 1.01451 | Validation loss = 0.56513 \n",
      "Epoch 13 | Training loss = 0.79373 | Validation loss = 2.41699 \n",
      "Epoch 14 | Training loss = 0.89427 | Validation loss = 0.44529 \n",
      "Epoch 15 | Training loss = 0.69761 | Validation loss = 0.84663 \n",
      "Epoch 16 | Training loss = 0.59522 | Validation loss = 0.39104 \n",
      "Epoch 17 | Training loss = 0.68214 | Validation loss = 0.46942 \n",
      "Epoch 18 | Training loss = 0.63042 | Validation loss = 0.73973 \n",
      "Epoch 19 | Training loss = 0.46434 | Validation loss = 0.32031 \n",
      "Epoch 20 | Training loss = 0.51622 | Validation loss = 0.24956 \n",
      "Epoch 21 | Training loss = 0.37946 | Validation loss = 0.23940 \n",
      "Epoch 22 | Training loss = 0.40806 | Validation loss = 0.32260 \n",
      "Epoch 23 | Training loss = 0.43845 | Validation loss = 0.16494 \n",
      "Epoch 24 | Training loss = 0.32409 | Validation loss = 0.19206 \n",
      "Epoch 25 | Training loss = 0.33482 | Validation loss = 0.20047 \n",
      "\n",
      "\n",
      "Training complete in 0m 37s\n"
     ]
    }
   ],
   "source": [
    "# To complete\n",
    "\n",
    "since = time.time()\n",
    "\n",
    "\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "n_epoch = 25\n",
    "\n",
    "model_rnn.load_state_dict(init_rnn_weights)\n",
    "\n",
    "print(\"Start training\")\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_n_iter = 0\n",
    "    \n",
    "    # Set model to train mode\n",
    "    \n",
    "    # Iterate over train data\n",
    "    for x, y in train_loader:  \n",
    "\n",
    "        \n",
    "        # Put tensors on device (GPU when available)\n",
    "        ... \n",
    "        \n",
    "        # Zero the gradient buffer\n",
    "        ...\n",
    "        \n",
    "        # Forward\n",
    "        outputs = ...\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(...)\n",
    "        \n",
    "        # Backward\n",
    "        ...\n",
    "        \n",
    "        # Optimize\n",
    "        ...\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        train_n_iter += 1\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_n_iter = 0\n",
    "    \n",
    "    # Set model to evaluate mode\n",
    "    ...\n",
    "    \n",
    "    # Iterate over valid data\n",
    "    for x, y in valid_loader:  \n",
    "        \n",
    "        # Put tensors on device (GPU when available)\n",
    "        ...\n",
    "        \n",
    "        # Forward\n",
    "        outputs = ...\n",
    "        \n",
    "        loss = criterion(...)\n",
    "        \n",
    "        # Statistics\n",
    "        valid_loss += loss.item()\n",
    "        valid_n_iter += 1\n",
    "    \n",
    "    train_loss_history.append(train_loss / train_n_iter)\n",
    "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
    "\n",
    "\n",
    "    print(\"Epoch {:2d} | Training loss = {:.5f} | Validation loss = {:.5f} \"\n",
    "          .format(epoch+1, (train_loss / train_n_iter), (valid_loss / valid_n_iter)))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "\n",
    "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr4at5nVhPXz"
   },
   "source": [
    "### Visualizing training curves \n",
    "\n",
    "Visualize the training curves using a graph of the cost function vs. epochs for both the training and the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "bJLu-rC_hPX1",
    "outputId": "1d0a653b-415d-4e44-eb6c-ea42426faf0c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save history for later\n",
    "rnn_train_loss_history = train_loss_history\n",
    "rnn_valid_loss_history = valid_loss_history\n",
    "\n",
    "# Plot training and validation curve\n",
    "xaxis = range(1, n_epoch + 1)\n",
    "plt.plot(xaxis, rnn_train_loss_history, label='train-rnn')\n",
    "plt.plot(xaxis, rnn_valid_loss_history, label='valid-rnn')\n",
    "\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vi7bHNzykdby"
   },
   "source": [
    "## Training an LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2RUJdIy72rZu"
   },
   "source": [
    "(This cell is reproduced from the equivalent RNN one above, skip if you have read/understood that one.)\n",
    "\n",
    "### Defining the cost and the optimizer\n",
    "\n",
    "Recall that a cost function  $J(\\theta) = L(x, y, \\theta)$ takes as input a prediction and the target and evaluates some distance (or discrepancy) between both.  For this example, we will use the mean squared error cost which is standard for regression problems (see [torch.nn.MSELoss()](https://pytorch.org/docs/stable/nn.html)):\n",
    "\n",
    "$J(\\cdot) = \\frac{1}{N}\\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^{2}$.\n",
    "\n",
    "\n",
    "To optimize the parameters of our networks we will use the *stochastic gradient descent* (SGD) optimizer. It minimizes the cost function $J(\\theta)$ parametrized by the networks' weights $\\theta$ by updating them using the following update rule: $\\theta \\leftarrow \\theta - \\alpha \\nabla J(\\theta)$, where  $\\alpha$ is the *learning rate*. The specificity of SGD is that ti will calculate the gradient $\\nabla$ using a single (or a small number of) example(s) instead of the full training data.\n",
    "\n",
    "In PyTorch we will use <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD()`</a> which is a SGD implementation. In this example, we will use a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pOZIqkBbpmeo"
   },
   "outputs": [],
   "source": [
    "# To complete\n",
    "criterion = ...\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqnTlRq8pmew"
   },
   "source": [
    "### Training the model\n",
    "\n",
    "To train out model, we will use our `train_loader` object to iterate over our entire training sets *n_epoch* times. \n",
    "To measure progress we will store the validation cost at the end of each training *epoch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "odcwM6DyhPYL",
    "outputId": "a548e4ab-25b2-4e14-f3c1-74227ae97166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Start training #\n",
      "Epoch  1 | Training loss = 17.85095 | Validation loss = 17.70797 \n",
      "Epoch  2 | Training loss = 6.09500 | Validation loss = 1.60636 \n",
      "Epoch  3 | Training loss = 1.49359 | Validation loss = 0.81062 \n",
      "Epoch  4 | Training loss = 0.64184 | Validation loss = 0.26455 \n",
      "Epoch  5 | Training loss = 0.47949 | Validation loss = 0.33894 \n",
      "Epoch  6 | Training loss = 0.36103 | Validation loss = 0.53644 \n",
      "Epoch  7 | Training loss = 0.34194 | Validation loss = 0.15094 \n",
      "Epoch  8 | Training loss = 0.24848 | Validation loss = 0.30115 \n",
      "Epoch  9 | Training loss = 0.21084 | Validation loss = 0.09204 \n",
      "Epoch 10 | Training loss = 0.19522 | Validation loss = 0.41419 \n",
      "Epoch 11 | Training loss = 0.16718 | Validation loss = 0.05944 \n",
      "Epoch 12 | Training loss = 0.16016 | Validation loss = 0.05129 \n",
      "Epoch 13 | Training loss = 0.14010 | Validation loss = 0.11142 \n",
      "Epoch 14 | Training loss = 0.10867 | Validation loss = 0.48988 \n",
      "Epoch 15 | Training loss = 0.12079 | Validation loss = 0.04461 \n",
      "Epoch 16 | Training loss = 0.09276 | Validation loss = 0.13373 \n",
      "Epoch 17 | Training loss = 0.09593 | Validation loss = 0.15664 \n",
      "Epoch 18 | Training loss = 0.08470 | Validation loss = 0.02725 \n",
      "Epoch 19 | Training loss = 0.07453 | Validation loss = 0.02499 \n",
      "Epoch 20 | Training loss = 0.05818 | Validation loss = 0.13851 \n",
      "Epoch 21 | Training loss = 0.05989 | Validation loss = 0.18455 \n",
      "Epoch 22 | Training loss = 0.07267 | Validation loss = 0.12572 \n",
      "Epoch 23 | Training loss = 0.05952 | Validation loss = 0.04904 \n",
      "Epoch 24 | Training loss = 0.05480 | Validation loss = 0.01633 \n",
      "Epoch 25 | Training loss = 0.06756 | Validation loss = 0.10727 \n",
      "\n",
      "\n",
      "Training complete in 1m 55s\n"
     ]
    }
   ],
   "source": [
    "# To complete\n",
    "\n",
    "since = time.time()\n",
    "\n",
    "\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "model_lstm.load_state_dict(init_lstm_weights)\n",
    "\n",
    "print(\"# Start training #\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_n_iter = 0\n",
    "    \n",
    "    # Set model to train mode\n",
    "    ...\n",
    "    \n",
    "    # Iterate over train data\n",
    "    for x, y in train_loader:  \n",
    "\n",
    "        \n",
    "        # Put tensors on device (GPU when available)\n",
    "        ...\n",
    "\n",
    "        # Zero the gradient buffer\n",
    "        ...\n",
    "        \n",
    "        # Forward\n",
    "        ...\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(...)\n",
    "        \n",
    "        # Backward\n",
    "        ...\n",
    "        \n",
    "        # Optimize\n",
    "        ...\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        train_n_iter += 1\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_n_iter = 0\n",
    "    \n",
    "    # Set model to evaluate mode\n",
    "    ...\n",
    "    \n",
    "    # Iterate over valid data\n",
    "    for x, y in valid_loader:  \n",
    "        \n",
    "        # Put tensors on device (GPU when available)\n",
    "        ...\n",
    "        \n",
    "        # Forward\n",
    "        outputs = ...\n",
    "        \n",
    "        loss = ...\n",
    "        \n",
    "        # Statistics\n",
    "        valid_loss += loss.item()\n",
    "        valid_n_iter += 1\n",
    "    \n",
    "    train_loss_history.append(train_loss / train_n_iter)\n",
    "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
    "    \n",
    "    print(\"Epoch {:2d} | Training loss = {:.5f} | Validation loss = {:.5f} \"\n",
    "          .format(epoch+1, (train_loss / train_n_iter), (valid_loss / valid_n_iter)))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "\n",
    "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aMEzsqBaqGmh"
   },
   "source": [
    "### Visualizing training curves \n",
    "\n",
    "Visualize the training curves using a graph of the cost function vs. epochs for both the training and the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "4Rw1tIfMhPYN",
    "outputId": "975035ea-6c59-40e7-d87d-fd4164d71211"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save history for later\n",
    "lstm_train_loss_history = train_loss_history\n",
    "lstm_valid_loss_history = valid_loss_history\n",
    "\n",
    "# Plot training and validation curve\n",
    "xaxis = range(1, num_epochs + 1)\n",
    "plt.plot(xaxis, lstm_train_loss_history, label='train-lstm')\n",
    "plt.plot(xaxis, lstm_valid_loss_history, label='valid-lstm')\n",
    "\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7PCowiQxpcHv"
   },
   "source": [
    "## Analyzing the results\n",
    "\n",
    "We will now compare the RNN and the LSTM using their performance on train/validation and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KaoQgOOZKL98"
   },
   "source": [
    "### Comparing training curves\n",
    "\n",
    "\n",
    "Compare the training curves using a graph of the cost function vs. epochs for both the training and the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "yl8hcNhnKKOS",
    "outputId": "8b1ae5b0-4138-4c63-f8b9-d117070e5842"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and validation curve\n",
    "xaxis = range(1, num_epochs + 1)\n",
    "\n",
    "plt.plot(xaxis, rnn_train_loss_history, label='train-rnn')\n",
    "plt.plot(xaxis, rnn_valid_loss_history, label='valid-rnn')\n",
    "\n",
    "plt.plot(xaxis, lstm_train_loss_history, label='train-lstm', linestyle='--')\n",
    "plt.plot(xaxis, lstm_valid_loss_history, label='valid-lstm', linestyle='--')\n",
    "\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.ylim([0,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BhGBAmabKr8A"
   },
   "source": [
    "Note that there is a different between the RNN and LSTM results.\n",
    "\n",
    "**Questions**\n",
    "\n",
    "*   Which network works best according to validation? Why is that? \n",
    "*   What is the most important different between both networks? \n",
    "*   What factors determine the smoothness of the curves? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeWGSfEOLdsM"
   },
   "source": [
    "### Evaluating test error\n",
    "\n",
    "Evaluate and print the mean squared error on the test set for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "72z2Xob3MRYB",
    "outputId": "4559f61c-4cfb-441c-e8ed-58388f1f3891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RNN's test mean squared error is 0.210\n",
      "The LSTM's trest mean squared error is 0.113\n"
     ]
    }
   ],
   "source": [
    "# To complete\n",
    "\n",
    "# Put data on device (GPU when available)\n",
    "xtest = ...\n",
    "ytest = ...\n",
    "\n",
    "ypred_rnn = ...\n",
    "ypred_lstm = ...\n",
    "\n",
    "loss_test_rnn = ...\n",
    "loss_test_lstm = ...\n",
    "\n",
    "\n",
    "print(\"The RNN's test mean squared error is %2.3f\" % float(loss_test_rnn))\n",
    "print(\"The LSTM's trest mean squared error is %2.3f\" % float(loss_test_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AmJDgK04NDVQ"
   },
   "source": [
    "\n",
    "**Question**\n",
    "\n",
    "*   Are test and validation results comparable? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hPZH71eCpcH0"
   },
   "source": [
    "### Further exploring the results\n",
    "\n",
    "To gain insights into our models we will inspect their outputs (i.e., their predictions before calculating the cost). To help us we will define a `print_sequence()` function. \n",
    "\n",
    "This function takes as input the tensors X and Y, samples a particular entry (sequence), prints the entry and prints the absolute difference between the true Y and the predicted Y.\n",
    "\n",
    "Recall that we have standardized our examples. For these visualizations we wish to use the original data pre-standardization. To do so, we use `mean` + `std` such that $xtest\\_unstd = xtest*std + mean$ et $ytest\\_unstd = ytest*std+seq\\_len*mean$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QfCvsGXNpcH2"
   },
   "outputs": [],
   "source": [
    "def print_sequence(X, Y, idx=0):\n",
    "    \"\"\"Print ground truth sum and predicted values.\n",
    "    Args:\n",
    "      X: torch.Tensor.\n",
    "      Y: torch.Tensor.\n",
    "      idx: index of the sequence (entry) to use.\n",
    "    \"\"\"\n",
    "    x = X[idx].numpy()\n",
    "    y = Y[idx].numpy()\n",
    "    for i, xi in enumerate(x):\n",
    "        if i==0:\n",
    "            string = str(xi[0]) \n",
    "        else:\n",
    "            string += \" + \" + str(xi[0])\n",
    "            \n",
    "    print(\"Sequence: \", string)\n",
    "    print(\"Prediction: \", str(y[0]))\n",
    "    print(\"Ground truth value: \", str(np.sum(x)))\n",
    "    diff = abs(np.sum(x)-y[0])\n",
    "    print(\"Absolute error between X[{a}] et Y[{a}]: {b}\".format(a=idx, b=diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "_aDw139hpcH_",
    "outputId": "e680663c-e426-4018-e831-ee3a7faa674f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example RNN:\n",
      "\n",
      "Sequence:  71.0 + -93.0 + 67.0 + 91.0 + -37.0 + -52.0 + -38.0 + -9.0 + -73.0 + 23.0 + 69.0 + 97.0 + -9.0 + 17.0 + 91.0 + 40.0 + 46.0 + 0.0\n",
      "Prediction:  280.76157\n",
      "Ground truth value:  301.0\n",
      "Absolute error between X[657] et Y[657]: 20.238433837890625\n",
      "\n",
      "Example LSTM:\n",
      "\n",
      "Sequence:  71.0 + -93.0 + 67.0 + 91.0 + -37.0 + -52.0 + -38.0 + -9.0 + -73.0 + 23.0 + 69.0 + 97.0 + -9.0 + 17.0 + 91.0 + 40.0 + 46.0 + 0.0\n",
      "Prediction:  279.6105\n",
      "Ground truth value:  301.0\n",
      "Absolute error between X[657] et Y[657]: 21.389495849609375\n"
     ]
    }
   ],
   "source": [
    "xtest = xtest.cpu()\n",
    "ypred_rnn = ypred_rnn.cpu()\n",
    "\n",
    "# To complete\n",
    "xtest_unstd = ...\n",
    "ypred_unstd = ...\n",
    "\n",
    "idx = np.random.randint(len(ytest))\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Example RNN:\")\n",
    "print(\"\")\n",
    "\n",
    "print_sequence(xtest_unstd.detach(), ypred_unstd.detach(), idx=idx)\n",
    "\n",
    "\n",
    "\n",
    "xtest = xtest.cpu()\n",
    "ypred_lstm = ypred_lstm.cpu()\n",
    "\n",
    "xtest_unstd = xtest*std + mean\n",
    "ypred_unstd = ypred_lstm*std + seq_len*mean\n",
    "\n",
    "print(\"\")\n",
    "print(\"Example LSTM:\")\n",
    "print(\"\")\n",
    "\n",
    "print_sequence(xtest_unstd.detach(), ypred_unstd.detach(), idx=idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAM5UeILQb5l"
   },
   "source": [
    "** Bonus questions**\n",
    "\n",
    "* Can you suggest modifications to improve the results of the LSTM?\n",
    "* Redo the above analysis but instead of summing the input values try other operations (e.g., -, x, /, ...)\n",
    "* Compare the performance of the RNN and the LSTM for sequences of different length (i.e., change `seq_len`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1jHLBHErzbW"
   },
   "source": [
    "---\n",
    "# Task 2: Neural language model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7kMH3bm1ZDz"
   },
   "source": [
    "## Objective\n",
    "\n",
    "The objective of the second part of the tutorial is to learn about text generation using recurrent neural networks. In particular, we will learn a recurrent neural network using a small amount of textual data written by [Shakespeare]https://en.wikipedia.org/wiki/William_Shakespeare). Once this model is trained, we will use it to generate new text in the style of Shakespeare.\n",
    "\n",
    "**Note** that people often use the term RNN even when training an LSTM. RNN has become the generic term regardless of the type (in addition to LSTMs that are other variants that are commonly used such as [Gated Recurrent Units (GRUs)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)).\n",
    "\n",
    "The model we will use is a bit more complex than the one for task 1. Contrary to the previous section, all code is provided (i.e., there are no exercises to complete). We suggest you go through all of the code to ensure that you understand both its logic as it relates to text processing but also how to design and train an RNN for text generation. The code could be relatively easily adapted to other tasks that you may care about. \n",
    "\n",
    "Further, this notebook also contains some of the data pre-processing steps, in particular the ones that take our dataset and arranges it such that it can be used to learn a neural language model based on LSTMs.  \n",
    "\n",
    "Happy generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-TECjbTm3L20"
   },
   "source": [
    "## A bit more context to understand language modelling\n",
    "(this was already covered in class)\n",
    "\n",
    "A sequence of words $\\mathbf{s}$ can be represented as a sequence of $N$ discrete symbols (or lexical tokens) such that $\\mathbf{s} = (w_{1}, \\dots, w_{N})$, where $w_{t}$ is a word or a punctation mark. Each symbol can be represented by an integer corresponding to its index in the vocabulary $V$. $V$ contains all symbols for a particular task (the vocabulary is usually built from the dataset we use for a task). The objective of a language model is to estimate the (joint) probability of a sequence $p(\\mathbf{s}) = p(w_{1}, \\dots, w_{N})$, which can be decomposed as a product of conditional probabilities such that:\n",
    "\n",
    "\\begin{equation}\n",
    "  p(\\mathbf{s}) = \\prod^{N}_{t=1} p(w_{t} | w_{1}, \\dots, w_{t-1}).\n",
    "\\end{equation}\n",
    "(this is also known as the chain rule in probability theory)\n",
    "\n",
    "This is important for modelling. In particular, instead of modelling the joint distribution directly, we can \"simply\" model each conditional. That is we can model the probability of the next word given all previous words ($p(w_{t} | w_{<t})$). Language models do just that and are widely used across numerous applications (including in automatic translation, speech recognition, and information retrieval). Note that this can be understood as a multi-class classification problem where the classes corresponds to the different words.\n",
    "\n",
    "However, modelling each conditional is not easy. Instead we perform an approximation which makes the problem easier. The intuition behind the approximation is that instead of conditioning of the whole history we condition on a smaller history ($w_{t-1}, w_{t-2}, \\ldots, w_{t-n}$) to predict the next word ($w_{t}$). This is also known as a $n-1$-order Markov assumption. Mathematically it is:\n",
    "\\begin{equation}\n",
    "  p(w_{t} | w_{1}, \\dots, w_{t-1}) \\approx p(w_{t} | w_{t-n+1}, \\dots, w_{t-1}).\n",
    "\\end{equation}\n",
    "\n",
    "In the next paragraph we explain how to model the above conditionals using a recurrent neural network. \n",
    "\n",
    "The architecture of the neural language model we use is an LSTM networks which will learn at each timestep a conditional distribution of the next word using a number of previous words.\n",
    "To train its parameters, we first need to set the maximum number $n-1$ of previous words to consider (`seq_len` in the code)--this is the effective size of your history--for training $p(w_{t} | w_{t-n+1}, \\dots, w_{t-1})$. The input to the LSTM at each time step is: 1) the $t$'th word $w_t$ encoded using its *word embedding* (see below); 2) the recurrent state ($mathbf{h}_{t-1}$); and 3) the state of the memory at the previous timestep ($\\mathbf{c}_{t-1}$).\n",
    "\n",
    "In addition the output of the LSTM at each step is the next word $w_{t+1}$, that is we train the LSTM to predict the next word at each step (in details the LSTM will actually predict the probability of that next word). This also implies that once this model is trained we will be able to use it to generate text (we will simply pick the word with the highest probability and feed it as the input to the next step). This type of model is commonly known as an *LSTM-based neural network model*. Its architecture is shown below:\n",
    "\n",
    "![alt-text](https://github.com/nextai-mtl/tech-2019/blob/master/images/autoregressive_english.png?raw=true)\n",
    "\n",
    "To compute the probability over all next words we simply use a *softmax* activation function. The softmax function returns a normalized $|V|$-dimensional vector, where each entry corresponds to a single word in the vocabulary. Each entry can be understood as the \"probability\" that the next word should the word at this index.\n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "The main question that remains is how exactly to encode the words at each timestep. \n",
    "Since the input data to a neural network must be encodable in a matrix, each symbol (word) $w_t$ in the vocabulary can be represented as a *one-hot* vector $\\mathbf{x}_i$ which is a vector of zeros with a single 1 at the position of the index of this word in the vocabulary. These one-hot vectors are multiplied by a weight matrix $\\mathbf{E} \\in \\mathbb R^{|V| \\times d_{e}}$. This matrix is learned and is known as the *embedding matrix*, $\\mathbf{E} \\in \\mathbb R^{|V| \\times d_{e}}$, it effectively encodes words in a continuous representation: $\\mathbf{w}_{t} \\in \\mathbb R^{d_{e}}$.\n",
    "\n",
    "Using word embeddings, a sequence of words can therefore be represented a sequence of size-$N$ vectors $\\mathbf{s} = (\\mathbf{w}_{1}, \\dots, \\mathbf{w}_{N})$. Each line $i$ of this matrix $\\mathbf{E}$ is a representation in $d_{e}$ dimensions of the $i$'th word in the vocabulary $V$. As we said above, these representations are often called *word embeddings*. When they are learned from a large enough dataset, they can represent semantic similarity. [For more information regarding word embeddings](http://ruder.io/word-embeddings-1/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u1Sh2OG46_Ln"
   },
   "source": [
    "## Utility functions for processing text and data structuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sf7luc-Di7qO"
   },
   "source": [
    "To train our neural language model we will use a dataset (corpus) of the works of Shakespeare (we used a subset from this link [https://norvig.com/ngrams/] which was cleaned, tokenized and standardized) available in the file `shakespeare_top20K.txt`.\n",
    "\n",
    "To obtain reasonable results in practice we should train a language model using a very-large quantity of text. In this tutorial, we will use a relatively small corpus of 20000 sentences, 159884 tokens, and a vocabulary of 12354 tokens. A token is a lexical unit separated by a whitespace on each side in the text. In our case, a token is a word, a number or a punctuation mark. The vocabulary is the set of all tokens in a corpus. (Of course, our code generalizes to larger datasets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "yo7yRlDyK7DA",
    "outputId": "f3ddb94a-dd8c-4df8-8c10-5d704db2f536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ecole_dl_mila_ivado_2018_10'...\n",
      "remote: Enumerating objects: 214, done.\u001b[K\n",
      "remote: Total 214 (delta 0), reused 0 (delta 0), pack-reused 214\u001b[K\n",
      "Receiving objects: 100% (214/214), 3.78 MiB | 2.95 MiB/s, done.\n",
      "Resolving deltas: 100% (73/73), done.\n"
     ]
    }
   ],
   "source": [
    "# Clone the git repo to obtain the data\n",
    "!git clone https://github.com/nextai-mtl/tech-2019.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pjv24-kr528"
   },
   "outputs": [],
   "source": [
    "START_VOCAB = [\"_UNK\"]\n",
    "UNK_ID = 0\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sdH5sXVsDgU"
   },
   "outputs": [],
   "source": [
    "def create_vocabulary(corpus_path, vocab_path, max_vocab_size=1e5):\n",
    "    \"\"\"Create and save the vocabulary of a corpus.\"\"\"\n",
    "    vocab = {}\n",
    "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split()\n",
    "            for token in tokens:\n",
    "                if token in vocab:\n",
    "                    vocab[token] += 1\n",
    "                else:\n",
    "                    vocab[token] = 1\n",
    "    vocab_list = START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "    if len(vocab_list) > max_vocab_size:\n",
    "        vocab_list = vocab_list[:max_vocab_size]\n",
    "    with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for token in vocab_list:\n",
    "              f.write(token + \"\\n\")\n",
    "\n",
    "\n",
    "def initialize_vocabulary(vocab_path):\n",
    "    \"\"\"Initialize the vocabulary.\"\"\"\n",
    "    if os.path.exists(vocab_path):\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            rev_vocab = [line.strip() for line in f.readlines()]\n",
    "        vocab = dict([(w, i) for (i, w) in enumerate(rev_vocab)])\n",
    "        return vocab, rev_vocab\n",
    "    else:\n",
    "        raise ValueError(\"Vocabulary file {} not found.\".format(vocab_path))\n",
    "\n",
    "\n",
    "def read_corpus(corpus_path):\n",
    "    \"\"\"Read and convert a corpus in a list of tokens.\"\"\"\n",
    "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        corpus = f.read().split()\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def corpus_to_token_ids(corpus, vocab):\n",
    "    \"\"\"Convert a corpus in token-ids.\"\"\"\n",
    "    token_ids = [vocab.get(token, UNK_ID) for token in corpus]\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "def batch_data(data, batch_size):\n",
    "    \"\"\"Structure the data in bath_size continuous sequences.\"\"\"\n",
    "    n_batch = len(data) // batch_size\n",
    "    data = np.array(data[:n_batch*batch_size])\n",
    "    data = data.reshape(batch_size, -1).T\n",
    "    return data\n",
    "\n",
    "\n",
    "def detach_hidden(hidden):\n",
    "    \"\"\"Transform the data from the hidden states of the LSTM\n",
    "       into a new Tensor with require_grad=False.\"\"\"\n",
    "    return tuple(h.detach() for h in hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the corpus: 159884\n",
      "\n",
      "Size of vocabulary: 12354\n",
      "\n",
      "Top 20 most frequent tokens in the corpus: \n",
      "[',', '.', 'the', 'I', ';', 'and', 'to', 'of', 'you', 'a', ':', 'in', '?', 'my', 'is', 'that', '!', 'not', 'it', 'me']\n",
      "\n",
      "First sentence in the corpus in text format:\n",
      "A MIDSUMMER-NIGHT'S DREAM Now , fair Hippolyta , our nuptial hour Draws on apace : four happy days bring in Another moon ; but O ! methinks how slow This old\n",
      "\n",
      "First sentence in the corpus in token-id format:\n",
      "[70, 5876, 5877, 194, 1, 143, 1948, 1, 58, 1425, 319, 4108, 46, 1949, 11, 607, 778, 608, 275, 12, 1950, 415, 5, 35, 62, 17, 1042, 119, 1115, 114, 173]\n",
      "\n",
      "Test conversaion from token-ids to text using rev_vocab:\n",
      "A MIDSUMMER-NIGHT'S DREAM Now , fair Hippolyta , our nuptial hour Draws on apace : four happy days bring in Another moon ; but O ! methinks how slow This old\n",
      "\n",
      "Structure the training data (note that the first sentence is in the first colon):\n",
      "tensor([[   70,  3426,  1967,    31,    51,     1,  4276,     4,    17,   113],\n",
      "        [ 5876,     1,     1,  4954,     2,     3,     2,   123,    38,    16],\n",
      "        [ 5877,  6562,     4,   262,   303,   101,   118,    51,  2018,    28],\n",
      "        [  194,  2799,   189,    51,    90,  4639,   375,    47,  1892,    50],\n",
      "        [    1,   534,     9,    43,     9,    16,     4,  1110,     7,  1544],\n",
      "        [  143,    12,    13,  1243,    39,   161,     1,     1,  5780,   589],\n",
      "        [ 1948,    10,   128,     1,    10,  3415,   390,   327,     2,   191],\n",
      "        [    1,  1086,     1,  7779,   641,   148,     8,    51,   102, 11814],\n",
      "        [   58,     8,    28,    51,   339,  3464,    21,    64,    14,     2],\n",
      "        [ 1425,  4440,    31,    43,     2,     2,   933,  4044,   130,    53],\n",
      "        [  319,    11,   175,   113,     4,   139,     9,     5,   230,    19],\n",
      "        [ 4108,    38,    98,  3734,    55,    56,   327,     6,     1,    15],\n",
      "        [   46,  4441,    27,  1611,    34,    81,   121,    12,     4,   513],\n",
      "        [ 1949,   201,  2469,     1,     5,   159,  1623,  5018,  2868,    16],\n",
      "        [   11,     1,     6,     6,     4,    65,    10,  2632,   603,   108],\n",
      "        [  607,    12,   497,   135,    37,   642,  9637,     1,   588,    28],\n",
      "        [  778,    10,     5,   174,    54,     7,     1,  5642,     2,   350],\n",
      "        [  608,   768,  1260,    51,    19,   459,    96,    98,     4,  1467],\n",
      "        [  275,  3427,     3,   277,   180,     2,  9638,    51,    26,    27],\n",
      "        [   12,  6563,  7207,    28,    77,    53,   110,   187,   374, 11815]])\n"
     ]
    }
   ],
   "source": [
    "# Create and initialize the vocabulary\n",
    "corpus_path = \"tech-2019/data/shakespeare_top20K.txt\"\n",
    "vocab_path = \"vocab.txt\"\n",
    "\n",
    "create_vocabulary(corpus_path, vocab_path)\n",
    "vocab, rev_vocab = initialize_vocabulary(vocab_path)\n",
    "\n",
    "# Read the training corpus\n",
    "corpus = read_corpus(corpus_path)\n",
    "token_ids = corpus_to_token_ids(corpus, vocab)\n",
    "\n",
    "# Structure the corpus in continuous sequences of batch_size for training\n",
    "batch_size = 10\n",
    "data = batch_data(token_ids, batch_size)\n",
    "data = torch.LongTensor(data).to(DEVICE)\n",
    "\n",
    "print(\"Number of tokens in the corpus: {}\"\n",
    "      .format(len(corpus)), end=\"\\n\\n\")\n",
    "print(\"Size of vocabulary: {}\"\n",
    "      .format(len(vocab)), end=\"\\n\\n\")\n",
    "print(\"Top 20 most frequent tokens in the corpus: \\n{}\"\n",
    "      .format(rev_vocab[1:21]), end=\"\\n\\n\")\n",
    "print(\"First sentence in the corpus in text format:\\n{}\"\n",
    "      .format(\" \".join(corpus[:31])), end=\"\\n\\n\")\n",
    "print(\"First sentence in the corpus in token-id format:\\n{}\"\n",
    "      .format(token_ids[:31]), end=\"\\n\\n\")\n",
    "print(\"Test conversaion from token-ids to text using rev_vocab:\\n{}\"\n",
    "      .format(\" \".join([rev_vocab[i] for i in token_ids[:31]])), end=\"\\n\\n\")\n",
    "print(\"Structure the training data (note that the first sentence \"\n",
    "      \"is in the first colon):\\n{}\".format(data[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "E2EXx8HBsZAY",
    "outputId": "3d2834be-4806-4eb5-bd9d-9b790c4eae87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble d'entraînement: 14389 séquences de longueur 40 et 360 minibatches\n",
      "Ensemble de validation : 1599 séquences de longueur 40 et 40 minibatches\n"
     ]
    }
   ],
   "source": [
    "# Split ensembles training/validation and create DataLoaders\n",
    "X = data[:-1]\n",
    "Y = data[1:]\n",
    "\n",
    "n_valid = round(data.size(0) * 0.1)\n",
    "train_set = TensorDataset(X[:(data.size(0)-n_valid)], Y[:(data.size(0)-n_valid)])\n",
    "valid_set = TensorDataset(X[-n_valid:], Y[-n_valid:])\n",
    "\n",
    "seq_len = 40\n",
    "train_loader = DataLoader(train_set, batch_size=seq_len, shuffle=False)\n",
    "valid_loader = DataLoader(valid_set, batch_size=seq_len, shuffle=False)\n",
    "\n",
    "print(\"Training set: {} sequences of length {} and {} minibatches\"\n",
    "      .format(len(train_set), seq_len, len(train_loader)))\n",
    "print(\"Validation set: {} sequences of length {} and {} minibatches\"\n",
    "      .format(len(valid_set), seq_len, len(valid_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAgSaVwmVxrt"
   },
   "source": [
    "## Implementing the model\n",
    "\n",
    "Below you will find the code of an *LSTM-based neural language model* as described in the **A bit more context to understand language modelling** Section.\n",
    "\n",
    "At each timestep, the input variables of the LSTM are:\n",
    "<ol>\n",
    "    <li> a minibatch of sequences of token-ids (i.e., sequences of indices where each index represents the position of a token in the vocabulary);</li>\n",
    "    <li> tuples $(\\mathbf{h}_{0}, \\mathbf{c}_{0})$ corresponding to recurrent states and memory states  $(\\mathbf{h}_{T}, \\mathbf{c}_{T})$ from the previous minibatch  (except for the first minibatch where we simply initialize these variables to 0.0 with the `init_hidden()` function).\n",
    "</ol>\n",
    "\n",
    "Each sequence of token-ids is transformed in sequences of *word embeddings* by indexing the *word embeddings* created by the class [torch.nn.Embedding()](https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding) which is a matrix of parameters of dimension $|V| \\times d_{e}$, where $|V|$ is the vocabulary size (`vocab_size`) and $d_{e}$ is the dimension of a *word embedding* (`embedding_size`).\n",
    "\n",
    "We will also apply dropout on the input *word embeddings* as well as on the output layer to provide regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YMiwftasb4g"
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    \"\"\"LSTM based language neural model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, input_size, hidden_size, n_layers=1, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          vocab_size: size of the vocabulary.\n",
    "          input_size: size of the word embeddings.\n",
    "          hidden_size: size of the hiden states of the LSTM.\n",
    "          n_layers: number of layers of the LSTM (default: 1).\n",
    "          dropout: if non-zero, uses a layer of dropout at the input at at the output of \n",
    "                    the LSTM introduit une couche dropout à l'entrée et à la sortie,\n",
    "                    where the input sets the value of the dropout (default: 0.5).\n",
    "        \"\"\"        \n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, input_size)\n",
    "        self.lstm = nn.LSTM(input_size,\n",
    "                            hidden_size,\n",
    "                            n_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embeddings = self.dropout(self.embeddings(input))\n",
    "        output, hidden = self.lstm(embeddings, hidden)\n",
    "        output = self.dropout(output)\n",
    "        result = self.linear(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return result.view(output.size(0), output.size(1), result.size(1)), hidden\n",
    "\n",
    "    def init_weights(self):\n",
    "        init.uniform_(self.embeddings.weight, -0.1, 0.1)\n",
    "        init.xavier_uniform_(self.linear.weight, init.calculate_gain(\"linear\"))\n",
    "        init.constant_(self.linear.bias, 0)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize the values of the hidden state and the LSTM cell to zero.\n",
    "        Args:\n",
    "          batch_size: size of a mini-batch at one time-step\n",
    "          \n",
    "        Returns:\n",
    "          hidden: hidden state h_t and the cell c_t à t=0 initialized to 0, \n",
    "                  ((n_layers, batch_size, hidden_size),\n",
    "                   (n_layers, batch_size, hidden_size)).\n",
    "        \"\"\"\n",
    "        hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_size, device=DEVICE),\n",
    "                  torch.zeros(self.n_layers, batch_size, self.hidden_size, device=DEVICE))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oPPkC_0GV3iu"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "The training procedure is very similar to the procedure used in Task 1. The key elements that differ are: \n",
    "\n",
    "* We use dropout on the input *word embeddings* as well as on the output layer of the LSTM. The small size of our data makes regularization important. In particular, for language models based on recurrent neural networks, we suggest using a fairly large droput value.\n",
    "* We change the learning rate through training. We begin with a fairly high one (see `learning_rate`) and divide it by 10 as a function of the validation error at the end of each *epoch*. In particular we use this class:  [torch.optim.lr_scheduler.ReduceLROnPlateau()](https://pytorch.org/docs/stable/optim.html?highlight=plateau#torch.optim.lr_scheduler.ReduceLROnPlateau).\n",
    "* To mitigate the problem of *exploding gradient* we use a *gradient-clipping* approach and normalize the norm of the gradient using this function [torch.nn.utils.clip_grad_norm_()](https://pytorch.org/docs/stable/nn.html?highlight=clip#torch.nn.utils.clip_grad_norm_).\n",
    "* We initialize to 0.0 the values of the tuple containing the recurrent state and the hidden memory state $(\\mathbf{h}_{0}, \\mathbf{c}_{0})$ using the `init_hidden()` function. We do this only once at the beginning of each *epoch* and we propagate the new values of hidden through all training minibatches. In other words, the data has beeen structured using the `batch_data()` function such that we can initialize `hidden` $(\\mathbf{h}_{0}, \\mathbf{c}_{0})$ of each sequence of minibatch using $(\\mathbf{h}_{T}, \\mathbf{c}_{T})$ of the previous minibatch. Using this method implies that we cannot shuffle the order of the sequences at each training *epoch* (i.e., `train_loader = DataLoader(train_set, batch_size=seq_len, shuffle=False)`).\n",
    "* Our cost function is the cross-entropy [torch.nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/nn.html?highlight=crossentropy#torch.nn.CrossEntropyLoss) which is standard for multi-class classification problems.\n",
    "* We will use *[perplexity](https://en.wikipedia.org/wiki/Perplexity)* as a way to measure the quality of our model. The intuition is that a good model should not be perplex when evaluating new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "colab_type": "code",
    "id": "winuGw1rsuOe",
    "outputId": "9ff9c5f3-a3c1-4537-8692-0b9fcc629924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de paramètres dans le modèle: 9783354\n",
      "Entraînement du modèle pour 20 epochs de 360 minibatches\n",
      "Epoch  1 | Training loss = 6.74751 | Validation loss = 5.92905 | Perplexity = 375.80\n",
      "Epoch  2 | Training loss = 5.98986 | Validation loss = 5.73647 | Perplexity = 309.97\n",
      "Epoch  3 | Training loss = 5.77130 | Validation loss = 5.58605 | Perplexity = 266.68\n",
      "Epoch  4 | Training loss = 5.63721 | Validation loss = 5.52013 | Perplexity = 249.67\n",
      "Epoch  5 | Training loss = 5.53869 | Validation loss = 5.46346 | Perplexity = 235.91\n",
      "Epoch  6 | Training loss = 5.45660 | Validation loss = 5.41640 | Perplexity = 225.07\n",
      "Epoch  7 | Training loss = 5.37894 | Validation loss = 5.40622 | Perplexity = 222.79\n",
      "Epoch  8 | Training loss = 5.31645 | Validation loss = 5.37788 | Perplexity = 216.56\n",
      "Epoch  9 | Training loss = 5.25550 | Validation loss = 5.38792 | Perplexity = 218.75\n",
      "Epoch 10 | Training loss = 5.19939 | Validation loss = 5.36753 | Perplexity = 214.33\n",
      "Epoch 11 | Training loss = 5.14271 | Validation loss = 5.37240 | Perplexity = 215.38\n",
      "Epoch 12 | Training loss = 5.09691 | Validation loss = 5.37463 | Perplexity = 215.86\n",
      "Epoch 13 | Training loss = 5.05374 | Validation loss = 5.36404 | Perplexity = 213.59\n",
      "Epoch 14 | Training loss = 5.00831 | Validation loss = 5.38665 | Perplexity = 218.47\n",
      "Epoch 15 | Training loss = 4.96182 | Validation loss = 5.38813 | Perplexity = 218.79\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e+00.\n",
      "Epoch 16 | Training loss = 4.92698 | Validation loss = 5.39601 | Perplexity = 220.53\n",
      "Epoch 17 | Training loss = 4.76036 | Validation loss = 5.36740 | Perplexity = 214.31\n",
      "Epoch 18 | Training loss = 4.69335 | Validation loss = 5.36902 | Perplexity = 214.65\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 19 | Training loss = 4.65659 | Validation loss = 5.37491 | Perplexity = 215.92\n",
      "Epoch 20 | Training loss = 4.62498 | Validation loss = 5.37093 | Perplexity = 215.06\n",
      "Félicitations! Vous avez terminé d'entraîner votre beau modèle de langue neuronal!\n"
     ]
    }
   ],
   "source": [
    "# Construct the model\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 300\n",
    "hidden_size = 400\n",
    "n_layers = 1\n",
    "dropout = 0.65\n",
    "model = LanguageModel(vocab_size, embedding_size, hidden_size, n_layers, dropout).to(DEVICE)\n",
    "print(\"Number of parameters in the model:\", sum(param.nelement() for param in model.parameters()))\n",
    "\n",
    "# Cost and optimization functions\n",
    "learning_rate = 10\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "# Training the model\n",
    "n_epochs = 20\n",
    "max_grad_norm = 1\n",
    "\n",
    "print(\"Training the model for {} epochs of {} minibatches\".format(n_epochs, len(train_loader)))\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        # Separate the hidden nodes from the computational graph\n",
    "        hidden = detach_hidden(hidden)\n",
    "        \n",
    "        # Reinitialize the gradient\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        # Forward pass\n",
    "        y_pred, hidden = model(x, hidden)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = loss_fun(y_pred.view(-1, vocab_size), y.view(-1))\n",
    "        \n",
    "        # Calculate the gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normalise the gradient to prevent gradient expoding\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimizer.step()        \n",
    "        \n",
    "        # Accumulate total loss\n",
    "        train_loss += len(x) * loss.item()\n",
    "    \n",
    "    # Eval the model using the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        for x, y in valid_loader:\n",
    "            y_pred, hidden = model(x, hidden)\n",
    "            loss = loss_fun(y_pred.view(-1, vocab_size), y.view(-1))\n",
    "            valid_loss += len(x) * loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    print(\"Epoch {:2d} | Training loss = {:.5f} | Validation loss = {:.5f} | Perplexity = {:.2f}\"\n",
    "          .format(epoch+1, train_loss, valid_loss, np.exp(valid_loss)))\n",
    "print(\"Congratulations! You have now trained a neural language model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKAOoUwWV8To"
   },
   "source": [
    "## Text generation\n",
    "\n",
    "Now that we have trained a language model, we can use it to generate text as Shakespeare! \n",
    "To do so, we will randomly select the first word (i.e., a token in a vocabulary) and use that as the input token at timestep one. We will then use the output token at the first timestep as the input token at the second timestep and son on. In total we will generate `n_words`. The `smoothing` variable allows one to obtain change the diversity of the generated text. A higher value generate more diverse but often lower quality text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "lLg-dbOSsuVu",
    "outputId": "6ab42a51-681f-402d-cb3a-90649a13c8b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman in this time , And , by the heart , that I have heard . \n",
      " I am not a man . I say , and I will tell you . \n",
      " I am a fool , sir . I will do you , sir , I \n",
      " will entreat you . Come , sir , I am a woman . You shall \n",
      " have a simple word , sir ; but I am a poor man , and \n",
      " his mother is a good lady . I beseech you , sir , I know \n",
      " not . And , I will tell you , sir , we are no more \n",
      " . You shall not be a good word . I am no more . You \n",
      " have been a courtier , sir , as I have a good friend , and \n",
      " you do not know , To say , what you shall not be so , \n",
      " and my good lady , I charge you , sir , I will not be \n",
      " found to be a very good friend . I will not be a bawd , \n",
      " sir ; but I will tell you , for that , as I have heard \n",
      " him a word with her , as I have heard of your estate . I \n",
      " will not do some man no harm . But I am going to be a \n",
      " man of the forest , is the thing that I have , sir , I \n",
      " do not know you shall be a poor fellow . I would not were it \n",
      " with a maid . I am yours , I am sure you have not seen \n",
      " , as I am to eat the man . I will be wi' you , \n",
      " sir , as he is , my lord . I have seen you ,\n"
     ]
    }
   ],
   "source": [
    "# Generating sequences of words\n",
    "model.eval()\n",
    "x = torch.randint(0, vocab_size, (1, 1), dtype=torch.long, device=DEVICE)\n",
    "words = [rev_vocab[x]]\n",
    "n_words = 300\n",
    "smoothing = 0.5\n",
    "with torch.no_grad(): \n",
    "    hidden = model.init_hidden(1)\n",
    "    for i in range(n_words-1):\n",
    "        output, hidden = model(x, hidden)\n",
    "        weights = output.squeeze().div(smoothing).exp()\n",
    "        word_idx = torch.multinomial(weights, 1)\n",
    "        x.fill_(word_idx.squeeze())\n",
    "        word = rev_vocab[word_idx]\n",
    "        words.append(word)\n",
    "        if (i+1) % 15 == 0:\n",
    "            words.append(\"\\n\")\n",
    "print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8HI-Elp2UhBY"
   },
   "source": [
    "What do you think of this generated text? What is remarkable is that many short sequences of words seem plausible. Longer phrases, however, usually do not make much sense. \n",
    "\n",
    "We note that both commas `,` and periods `.` appear very often in this generated text. This can be explain by the fact that they are the most frequent tokens in our (training) dataset. \n",
    "\n",
    "To obtain higher-quality texte would require training with a larger corpus. Further, the vocabulary and the style of Shakespeare may not be easiest to learn from. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lVeeLCaC29Mx"
   },
   "source": [
    "---\n",
    "## References\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "https://arxiv.org/abs/1803.08240"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNN_solution.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
